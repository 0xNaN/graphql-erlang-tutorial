[[schema]]

== GraphQL Schema

With a Mnesia database at our disposal, we next create a GraphQL
schema definition. This file describes the contract between the client
system and the server system. It is used by the GraphQL system to know
which queries are valid and which aren't.

In accordance with OTP design principles, we place this schema inside
a projects `priv` directory. Our GraphQL system can then refer to the
private directory of the application in order to load this schema when
the system boots up.

[[identity-encoding]]
=== Identity encoding

In GraphQL, you have a special type, _ID_, which is used to attach an
identity to objects. It is often construed as a "`PRIMARY KEY`" of
sorts on objects, such that you can refer to objects uniquely. The
rule is that a client must only treat an _ID_ as an opaque string
value and never parse on the string value. Thus, an _ID_ is
represented as a _String_, but you are not allowed to parse it as a
client.

To make this more obvious, the GraphQL people usually base64 encode
their ID-values. Furthermore, we have the problem that in Mnesia, our
rows IDs will be integers. This means we may have overlapping integers
between different types. To avoid problem, we use a common encoding in
GraphQL. The Starship with id 3 will be encoded as
`base64("Starship:3")` for example. And the planet Tatooine taken from
the <<system-tour>> is encoded as `base64("Planet:1")`. This
definition somewhat hides the implementation and also allows the
server backend to redefine IDs later for objects. Another use of the
encoding is that it can define what datasource a given came from, so
you can figure out where to find that object. It is highly useful in
migration scenarios.

The encoder is simple because we can assume the server provides valid
values:

[source,erlang]
----
include::{sw_core}/src/sw_core_id.erl[tags=idEncode]
----

The decoder is a bit more involved. It requires you to fail on invalid
inputs. We usually don't need to know what was invalid. We can simply
fail aggressively if things turns out bad. A debugging session will
usually uncover the details anyway as we dig into a failure.

[source,erlang]
----
include::{sw_core}/src/sw_core_id.erl[tags=idDecode]
----

=== The Node Interface

The Relay Modern specification contains a number of standards for how
to implement different parts of the GraphQL schema. One such standard
is the *Node* interface. This interface allows you to "`start from`"
any node in the graph which has an _id_ field. That is, every node
with identity can be a starting point for a query.

The interface is most often used as a way to cache-refresh objects you
have loaded a long time ago in order to make sure they have the right
kinds of data. The interface specification follows the standard
closely:

[source,graphql]
----
include::{sw_core}/priv/sw.schema[tags=nodeInterface]
----

.On Documentation

The Erlang version of GraphQL allows a certain extension by the Apollo
people. This extension allows you to use <<annotations>> in GraphQL
schemas to attach more information to particular objects of interest.
We use this for documentation. You can annotate almost anything with
`+description(text: "documentation")` which in turn attaches that
documentation to an entity in the Graph.

Multi-line comments are also possible by using a backtick (`) rather
than a quote symbol ("). These allows larger Markdown entries to be
placed in the documentation, which tends to be good for documentation
of APIs.

NOTE: You can't easily use a backtick (`) inside the multiline
quotations. This means you can't easily write pre-formatted code
sections unless you use indentation in the Markdown format. The choice
was somewhat deliberate in that there is a workaround currently, and
it tends to flow really well when you enter documentation by means of
the backtick character. A future version of the parser might redo this
decision.

=== Planets

Since we have planets in the mnesia database from the previous
section, we can define the GraphQL Schema for them as well. The
definition is quite straightforward given the Star Wars API we are
trying to mimic already contains all the important parts.

For brevity, we omit the documentation of each individual field for
now. Though a more complete implementation would probably include
documentation on each field to a fine detail.

[source,graphql]
----
include::{sw_core}/priv/sw.schema[tags=planetObject]
----

[[queries-and-mutations]]
=== Queries & Mutations

All GraphQL queries are either a _query_ or a _mutation_.footnote:[The
spec has toyed with the idea of adding more classes in additon to
queries and mutations. Most notably the concept of a _subscription_]
Correspondingly, the schema specification contains entries for two
(output) objects, which are commonly called `Query` and `Mutation`
respectively. For example, the query object looks like:

[source,graphql]
----
include::{sw_core}/priv/sw.schema[tags=queryObject]
----

The Query object defines the (public) API of your backend. All queries
will start from here, and the specification defines what you can do
with the given query.

NOTE: The introspection capabilities of GraphQL will have the astute
reader recognize that this predefined rule of what you can do with a
query is very close to automatic discovery of capabilities. In other
words, you get close to the notion of <<HATEOAS>>, while not reaching
it.

The field `node` allows you to request any node in the Graph. Later,
we will see how to implement the backend functionality for this call.
In this example, we will request a *Starship* through the node
interface by first requesting anything implementing the *Node*
interface, and then use an inline-fragment in order to tell the system
which fields we want to grab inside the starship:

[source,graphql]
----
query StarShipQuery($id : ID!) {
    node(id: $id) {
       __typename
       id
       ... on Starship {
          model
          name
       }
    }
}
----

== Input objects

TBD

=== Schema default values

TBD

== Loading the Schema

In order to work with a schema, it must be loaded. We can load it as
part of booting the `sw_core` application in the system. After having
loaded the supervisor tree of the application, we can call out and
load the star wars schema into the system. The main schema loader is
defined in the following way:

[source,erlang]
----
include::{sw_core}/src/sw_core_app.erl[tags=loadSchema]
----

To load the schema, we figure out where it is in the file system. The
schema to load is in an environment variable inside `sw_core.app`, and
we let OTP figure out where the applications private directory is.
Then the schema is loaded according to the mapping rules of the
schema.

After the schema loads, we set up a _schema root_ which is how to
start out a _query_ or a _mutation_. Finally, we validate the schema.
This runs some correctness checks on the schema and fails of the
sanity checks doesn't pass. It forces you to define everything you
use, and it also verifies that interfaces are correctly implemented.

NOTE: Currently, the schema root is set up "`manually`" outside the
schema definition. It is likely that a later version of the
implementation will be able to do this without manually injecting the
root, but by having the root being part of the schema definition.

TIP: Always run the schema validator once you've finished assembling
your schema. Many errors are caught automatically by the validator,
and it removes the hassle of debugging later. Also, it runs fairly
quickly, so run it as part of your systems boot phase. This ensures
your system won't boot if there is some kind of problem with your
schema definition. If you have a boot-test as part of your testing
framework or CI system, you should be able to use this as a "`schema
type checker`" and weed out some obvious definitional bugs.

=== Root setup

The root setup defines how a query begins by defining what type in the
schema specification is the root for queries and mutations
respectively. By convention, these types are always called `Query` and
`Mutation` so it is easy to find find the Root's entry points in the
Graph.

The query root must be injected into the schema so the GraphQL systems
knows where to start. This is done in the file `sw_core_app` in the
function `setup_root`:

[source,erlang]
----
include::{sw_core}/src/sw_core_app.erl[tags=setupRoot]
----

=== Mapping rules

The mapping rules of the GraphQL system defines how the types in the
schema maps onto erlang modules. Since many mapping can be coalesced
into one, there is the possibility of defining a `default` mapping
which just maps every unmapped object to the default.

All of the mappings goes from an atom, which is the _type_ in the
Schema you want to map. To an atom, which is the Erlang _module_
handling that particular schema type.

[source,erlang]
----
include::{sw_core}/src/sw_core_app.erl[tags=schemaMapping]
----

.Scalars

Every scalar type in the schema is mapped through the `scalar` mapping
part. It is quite common a system only has a single place in which all
scalars are defined. But it is also possible to split up the scalar
mapping over multiple modules. This can be useful if you have a piece
of code where some scalars naturally lives in a sub-application of some
kind.

.Interfaces & Unions

In GraphQL, two kinds of _abstract_ types are defined: interfaces and
unions. Interfaces abstract over concrete types which have some fields
in common (and thus the fields must also agree on types). Unions
abstract over concrete types that has nothing in common and thus
simply defines a heterogenous collection of types.

For the GraphQL system to operate correctly, execution must have a way
to take an abstract type and make it concrete. Say, for instance, you
have just loaded an object of type *Node*. We don't yet know that it
is a starship, but if the programmer fragment expands on
the *Starship*

[source,graphql]
----
query Q($nid : ID!) {
  node(id: $nid) {
    ... on Starship {
      model
    }
  }
}
----

we need to know if the concrete node loaded indeed _was_ a starship.
The type resolver is responsible for figuring out what concrete type
we have. Commonly, we map both the interface and union type resolver
to the same resolver.

The reason this needs to be handled by the programmer is because the
GraphQL system doesn't know about your representation. In turn, it
call back into your code in order to learn which type your
representation has.

.Objects

The mapping of objects is likely to have a special mapping for each
object type you have defined. This is because each kind of (output)
object tend to be different and require its own handler.

Note that it is possible to define the type of the object as an
`atom()` here. This is common in GraphQL for Erlang. You can write
definitions as either atoms or binaries. They are most often returned
as binaries at the moment, however.

NOTE: The choice of using binaries may turn out to be wrong. We've
toyed with different representations of this, and none of them fell
out like we wanted. However, because the nature of GraphQL makes sure
that an enemy cannot generate arbitrary atoms in the system, we could
use atoms in a later version of GraphQL. For now, however, most parts
of the system accepts atoms or binaries, and converts data to binaries
internally.
